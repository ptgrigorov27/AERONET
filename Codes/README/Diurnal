# Read and Map AERONET

The purpose of this code is to compute and visualize diurnal and seasonal variability of aerosol optical depths, Angstrom
exponent parameters, and precipitable water. Similarly to the other codes, the data is read directly from AERONET's web
services. Different parameters can be specified by user such as date range, site name, season (optional), data level, AOD
wavelength, and Angstrom exponent. The output of the code is excel tables where the rows represent hourly bins (integer format,
values between 0 and 23), and the columns represent the average AOD, Angstrom exponent, and water vapor measurements. The code
also outputs diurnal variability plots for each specified AERONET site. The main use of this tool is to see how the AOD/AE/WV
measurements change throughout the day, or how they vary at a particular hour in the day. The either for a year-round diurnal
variability or for a given season.

## Table of Contents

- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
  - [Example 1](#example-1)
  - [Example 2](#example-2)
- [Features](#features)
- [How It Works](#how-it-works)
- [Extra Features](#extra-features)
- [Contributing](#contributing)

## Getting Started

### Prerequisites
Running this code is straightforward. The only requirements are having the means to run a Python notebook file
(.ipynb). This can be done either in Google Colab (recommended), if the user has a Google account. It can also 
be done locally on Jupyter Notebook, which requires installation of Anaconda: https://www.anaconda.com/

### Installation

There is a list of libraries that need to be installed before running the code. Those include: cartopy, beautifulsoup4,
shapely, requests, and geopandas. All other libraries used are already installed on Google Colab/Anaconda by default.
Note that the exclamation point is not required when installing through Anaconda command prompt.

!pip install beautifulsoup4
!pip install requests
!pip install timezonefinder
!pip install geopy

The beautifulsoup4 package provides web scraping capabilities, where data from a webpage can be read into "soup" file that 
can be later converted to a dataframe. The requests package is useful for sending HTTP requests using Python, in this case to
obtain URL of web data. The "timezonefinder" package was useful for calculating time differences between GMT time (default
timestamp for data) and the local time of the site (computed using geopy package using lat/lon). Knowing the local time is 
integral to calculating the solar times of the sites, which ensures accurate diurnal variability plots.

In addition to the packages discussed above, the code also makes use of libraries like shutil, numpy, math, datetime, pandas,
matplotlib, and sklearn. The shutil library was used for creating zip files with the output. Array manipulation was performed
with the numpy library, and time data manipulation was performed with the datetime library. The "math" library provides
access to mathematical functions such as standard deviations. The pandas library's purpose was to clean and aggregate data
for analysis. Diurnal variability plots were created with matplotlib.

## Usage

Below are the two cells which require action from the user. The first cell prompts user to allow Google to mount their local
drive onto the colab notebook. The user will need to allow access from the popup window that shows after running that cell. 

### Example 1

from google.colab import drive
drive.mount('/drive')

The second cell prompts the user for input. The list of parameters are initial and final dates of interest, season (optinal),
level of data, site name, AOD wavelength, and Angstrom exponent parameter. Default values have been setup in case a user 
provides an invalid response. Warnings have been put in place in case a user selects a timeframe that is possibly too large 
for the program to handle. If an URL cannot be generaed from the web services, the user is promped to try again.

### Example 2

dt_initial = '20230626'         #starting date YYYYMMDD format
dt_final = '20230630'           #final date YYYYMMDD format
season = 'spring'               #Available choices: spring, summer, autumn, winter, all
level = 1.5                     #AERONET data level
site = 'Tel-Aviv_University'    #AERONET location
wavelength = 500                #Available choices: 1640, 1020, 870, 865, 779, 675, 667, 620, 560, 555, 551, 532, 531, 510, 500, 490, 443, 440, 412, 400, 380, 340
Angstrom_exp = '440-870'        #Available choices: '440-870','380-500','440-675','500-870','340-440','440-675'

## How It Works

The code initially takes some of the user input parameters from Example 2, and uses them in constructing a URL that resembles
the real link that contains the data with those desired parameters. The URL is converted to a tuple to remove the commas, and
an HTTP request is made from Google Colab to get the text from the real URL using the tuple. If there is a match with the URL,
the beautiful soup package is used to scrape the information from the webpage. That information is saved as a text file and 
then read as a Pandas dataframe. The first few rows are skipped so that only the data and headers remain. The dataframe is 
processed according to the following mathematical algorithm:

  1. Calculate timezone of the site of interest, and taking the time difference from GMT to calculate local standard time.
  2. Calculate solar time using local standard time (daylight savings not included), standard meridian, longitude, and
     equation of time (function of day of year, which is found in web data).
  3. Creating hourly bins by truncating minutes and seconds from timestamp.
  4. Filtering dataframe based on seasons (OPTIONAL)
  5. Compute daily AOD/AE/WVC averages if there are 10 or more records per day (drops days with less than 10 records).
  6. Takes the difference between each instantaneous AOD/AE/WVC value and corresponding daily average.
  7. Computes mean and standard deviation of those differences for each hourly bin, PER day.
  8. Computes number of differences for each hourly bin, per day.
  9. Creates final table with the hourly bin averages, across all days. 
  10. Plots the table contents using matplotlib.

## Extra Features

The Diurnal_MultipleAOD code is adjusted so that a list of AOD wavelengths can be used as input, instead of just 1 value. 
Separate columns and separate graphs are created for each AOD wavelength. 

A second algorithm is used where the standard deviations and mean of differences are taken with respect to just hourly bin,
not hour and day. The number of differences are then counted by day, instead of day and hour.


## Contributing
The code is for demonstration purposes only. Users are responsible to check for accuracy and revise to fit their objective.
Report any concern or question related to the code to pawan.gupta@nasa.gov or petar.grigorov@nasa.gov
